import numpy as np


class NeuralNetwork:
    def __init__(self, neural_network_structure, activation_functions_list, minimum=0, maximum=1, bias=1):
        self.neural_network_ = neural_network_structure
        self.weights_ = self.random_weights(maximum, minimum)
        self.bias_ = bias
        self.activation_functions_list_ = activation_functions_list

    def __str__(self):
        st = ''
        for i in self.neural_network_:
            st += str(i) + '  '
        return st

    def neuron_value(self, value_arr, weight_arr, act_func):
        x = np.dot(value_arr, np.transpose(weight_arr))
        res = act_func(x)
        return res

    def neuron_value_list(self, input_data):
        number_of_iterations = len(self.activation_functions_list_)

        value_list = []

        for i in range(number_of_iterations):
            def f(x):
                if self.activation_functions_list_[i] == 'identity':
                    return x
                elif self.activation_functions_list_[i] == 'sigmoid':
                    return 1/(1+np.exp(-x))
                elif self.activation_functions_list_[i] == 'th':
                    return (np.exp(x)-np.exp(-x))/(np.exp(x)+np.exp(-x))

            if i == 0:
                value = np.append(f(input_data), self.bias_)
                value_list.append(value)
            elif i == number_of_iterations - 1:
                value = self.neuron_value(value, self.weights_[i - 1], f)
                value_list.append(value)
            else:
                value = np.append(self.neuron_value(value, self.weights_[i - 1], f), self.bias_)
                value_list.append(value)

        return value_list

    def backpropagation(self, value_list, expected_results, education_speed):
        weight_list_reversed = self.weights_[::-1]
        value_list_reversed = value_list[::-1]
        activation_functions_list_reversed = self.activation_functions_list_[::-1]

        delta_w_list = []

        for i in range(len(value_list_reversed)):
            def derivative(layer_output):
                if activation_functions_list_reversed[i] == 'identity':
                    return 1
                elif activation_functions_list_reversed[i] == 'sigmoid':
                    return (1 - layer_output) * layer_output
                elif activation_functions_list_reversed[i] == 'th':
                    return 1 - (layer_output * layer_output)

            if i == 0:
                delta = (expected_results - value_list_reversed[i]) * derivative(value_list_reversed[i])
            else:
                gradient = np.matmul(np.transpose(np.array([delta])), np.array([value_list_reversed[i]]))
                delta_w = education_speed * gradient
                delta_w_list.append(delta_w)
                delta = (np.sum(delta * np.transpose(weight_list_reversed[i - 1]), axis=1) * derivative(value_list_reversed[i]))[:-1]

        return delta_w_list[::-1]

    def random_weights(self, minimum, maximum):
        weights_list = []

        for i in range(len(self.neural_network_) - 1):
            weights = np.random.uniform(minimum, maximum, (self.neural_network_[i + 1], self.neural_network_[i] + 1))
            weights_list.append(weights)

        return weights_list

    def train(self, input_data, expected_result, number_of_repetitions=1000, education_speed=0.01):
        number_of_iterations = len(input_data)

        for j in range(number_of_repetitions):

            for i in range(number_of_iterations):
                value_list = self.neuron_value_list(input_data[i])
                delta_w = self.backpropagation(value_list, expected_result[i], education_speed)
                for k in range(len(self.weights_)):
                    self.weights_[k] += delta_w[k]

    def check(self, input_data):
        result = self.neuron_value_list(input_data)[-1]
        return result


def error_func(obtained, expected):
    res = 0
    for i in range(len(obtained)):
        res += (obtained[i] - expected[i]) ** 2
        res /= len(obtained)
        return res
